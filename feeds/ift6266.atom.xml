<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>seaandsailor</title><link href="/" rel="alternate"></link><link href="/feeds/ift6266.atom.xml" rel="self"></link><id>/</id><updated>2014-03-31T14:00:00-04:00</updated><entry><title>Using an auditory-inspired representation for speech</title><link href="/gammatone.html" rel="alternate"></link><updated>2014-03-31T14:00:00-04:00</updated><author><name>jfsantos</name></author><id>tag:,2014-03-31:gammatone.html</id><summary type="html">&lt;p&gt;I &lt;a class="reference external" href="http://www.seaandsailor.com/dict_learning.html"&gt;previously&lt;/a&gt;
described an approach to representing speech signals by decomposing
them to an arbitrary dictionary (using a sparse coding algorithm such
as Orthogonal Matching Pursuit). In that post, I showed that learning
a representation from the data by using a dictionary learning method
could be useful. However, there were some problems with that
approach. First, the dictionary atoms were not localized in time: the
atoms I learned from the data were waveforms spreading throughout the
entire frame. This behavior has led to issues when reconstructing the
signal, as nothing guarantees the last sample in the &lt;span class="math"&gt;\(k^{th}\)&lt;/span&gt;

frame will be close to the first sample in the &lt;span class="math"&gt;\(k+1^{th}\)&lt;/span&gt;

frame. The second issue was related to not using overlapped windows to
split/resynthesize the signal. This is one of the main reasons that
made the signals I generated previously so noisy.&lt;/p&gt;
&lt;p&gt;In order to solve these problems, I added two updates to my previous
code:&lt;/p&gt;
&lt;ol class="arabic simple"&gt;
&lt;li&gt;Dropping the dictionary I learned from the data and switched to a
gammatone dictionary.&lt;/li&gt;
&lt;li&gt;Generating the audio frames using Hamming windows with 50% overlap.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;In the next sections, I will give a brief description and motivation
for each of these updates. I will also show why they didn't work as
well as I expected and inspired another architecture.&lt;/p&gt;
&lt;div class="section" id="gammatone-functions-and-gammatone-based-dictionary"&gt;
&lt;h2&gt;Gammatone functions and gammatone-based dictionary&lt;/h2&gt;
&lt;p&gt;Gammatone filters are a popular way of modeling the auditory
processing at the cochlea. Basically, the cochlea is interpreted as a
filterbank whose impulse response follows the following equation (the
product of a &lt;em&gt;gamma&lt;/em&gt; function and a cosine, or a &lt;em&gt;pure tone&lt;/em&gt;):&lt;/p&gt;
&lt;div class="math"&gt;
\begin{equation*}
g(t) = at^{n-1}e^{-2\pi b t}\cos(2 \pi ft + \phi)
\end{equation*}
&lt;/div&gt;
&lt;p&gt;In this equation, &lt;span class="math"&gt;\(b\)&lt;/span&gt;
 corresponds to the filter's bandwidth,
&lt;span class="math"&gt;\(n\)&lt;/span&gt;
 is the filter order, &lt;span class="math"&gt;\(f\)&lt;/span&gt;
 is the central frequency, and
&lt;span class="math"&gt;\(\phi\)&lt;/span&gt;
 is the phase of the carrier. The first two parameters can
be fixed for the entire filterbank, while the center frequencies
&lt;span class="math"&gt;\(f\)&lt;/span&gt;
 are usually defined according to the cochlea's critical
frequencies. One way of computing these frequencies is by using the
Equivalent Rectangular Bandwidth (ERB), which gives an approximation
to the bandwidths of the human auditory filters:&lt;/p&gt;
&lt;div class="math"&gt;
\begin{equation*}
ERB_j = \frac{f_j}{Q_{ear}} + B_{min}
\end{equation*}
&lt;/div&gt;
&lt;p&gt;Here, &lt;span class="math"&gt;\(Q_{ear} = 9.26449\)&lt;/span&gt;
 and &lt;span class="math"&gt;\(B_{min} = 24.7\)&lt;/span&gt;
 are
constants corresponding to the Q factor and minimum bandwidth of human
auditory filters.&lt;/p&gt;
&lt;p&gt;Gammatone functions are used in auditory modelling because they match
the resonance of different regions in the cochlea. As shown in
&lt;a class="citation-reference" href="#smith2006" id="id1"&gt;[Smith2006]&lt;/a&gt;, human speech can be sparsely represented by gammatone
atoms. &lt;a class="citation-reference" href="#strahl2008" id="id2"&gt;[Strahl2008]&lt;/a&gt; has later shown that a sparse gammatone model can
be optimized for English speech, even though the optimized model does
not match the human auditory filters anymore.&lt;/p&gt;
&lt;p&gt;A gammatone dictionary can be built similarly to a Gabor dictionary,
as gammatones are localized both in time and frequency. First, we have
to choose a set of frequencies; usually, you pick the number of
frequencies you want and the range, and use the ERB equation to find
equally-spaced frequencies in the ERB space (these would be the so
called critical frequencies). Then, we select the resolution of our
atoms (which has to be less or equal to the frame length in our
application) and then time-shift the atoms inside the frame by a
specified amount. The following Python code does that (and also
normalizes the dictionary at the end):&lt;/p&gt;
&lt;pre class="code python literal-block"&gt;
&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;gammatone_matrix&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;b&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;fc&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;resolution&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;step&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="sd"&gt;&amp;quot;&amp;quot;&amp;quot;Dictionary of gammatone functions&amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;
     &lt;span class="n"&gt;centers&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;arange&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;resolution&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;step&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;step&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
     &lt;span class="n"&gt;D&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;empty&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;centers&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;resolution&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
     &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;center&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;enumerate&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;centers&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
         &lt;span class="n"&gt;D&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;gammatone_function&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;resolution&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;fc&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;center&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;b&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;b&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
     &lt;span class="n"&gt;D&lt;/span&gt; &lt;span class="o"&gt;/=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sqrt&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;D&lt;/span&gt; &lt;span class="o"&gt;**&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;axis&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;))[:,&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;newaxis&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
     &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;D&lt;/span&gt;
&lt;/pre&gt;
&lt;p&gt;For illustration, see below 5 time-shifted versions of the same
gammatone (note that in the actual dictionary, we probably want the
time-shifted atoms to overlap a bit more than in this figure).&lt;/p&gt;
&lt;img alt="" class="align-center" src="images/gammatones.png" style="width: 500px;" /&gt;
&lt;p&gt;See my gammatone sparse coding library &lt;a class="reference external" href="https://github.com/jfsantos/ift6266h14/blob/master/sparse_coding/sparse_coding_gammatone.py"&gt;here&lt;/a&gt;, and an updated version
of my IPython notebook for sparse coding &lt;a class="reference external" href="https://github.com/jfsantos/ift6266h14/blob/master/sparse_coding/Sparse%20coding%20with%20a%20multiscale%20Gammatone%20dictionary.ipynb"&gt;there&lt;/a&gt; for more details. The
test code in the library reads a wave file, segments it in 2048 frames
with 50% overlap, windows each frame with a Hanning window (see next
section for details) and decomposes each frame using gammatone
atoms. The reconstruction in this example uses 200 non-zero
coefficients per frame and the dictionary has 3150 atoms. This amounts
for a compression of more than 10 times, but the reconstruction does
not sound as bad as the ones we've seen previously.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="overlapping-windows"&gt;
&lt;h2&gt;Overlapping windows&lt;/h2&gt;
&lt;p&gt;A window function is a function that has non-zero values only inside a
given interval. The most classical example of it is the rectangular
window:&lt;/p&gt;
&lt;div class="math"&gt;
\begin{equation*}
w_{rect}[n] = \begin{cases} 1, \mbox{if } n_0 \leq n \leq n_f \\
0, \mbox{ otherwise} \end{cases}
\end{equation*}
&lt;/div&gt;
&lt;p&gt;However, a problem with the rectangular window is that it does nothing
to smooth the signal at the window borders. If we are processing a
signal on a per-frame basis and then reconstructing it by
concatenating the processed frames, nothing guarantees continuity when
we join the processed frames. These abrupt changes introduce broadband
noise bursts in our signal, which is something that we probably do not
want!&lt;/p&gt;
&lt;p&gt;A way to mitigate this problem is to do &lt;a class="reference external" href="https://ccrma.stanford.edu/~jos/parshl/Overlap_Add_Synthesis.html"&gt;overlap-add&lt;/a&gt;
synthesis. Instead of shifting a full frame at a time and using
rectangular windows, we overlap frames by a certain amount (25%, 50%,
and 75% are often used values) and multiply each frame by a smooth
window. We use window functions in such a way that the overlapped
windows always sum to unity. The figure below shows two &lt;a class="reference external" href="https://ccrma.stanford.edu/~jos/sasp/Hamming_Window.html"&gt;Hamming
windows&lt;/a&gt; with an overlap of 50% (blue and green curves), and the sum
of both windows (red curve). If we keep overlapping windows like this,
overlap-add is an identity operation (i.e., we do not change the final
result as long as we do not process the frames). Of course, in our
case we are processing the frames, but overlap-add will help a bit in
mitigating the abrupt changes between frames as we are now summing the
values in overlapping frames to reconstruct our output instead of just
connecting two non-overlapping frames.&lt;/p&gt;
&lt;img alt="" class="align-center" src="images/hamming_windows.png" style="width: 600px;" /&gt;
&lt;/div&gt;
&lt;div class="section" id="experiment-with-sparse-coding-using-gammatone-atoms"&gt;
&lt;h2&gt;Experiment with sparse coding using gammatone atoms&lt;/h2&gt;
&lt;p&gt;Based on the ideas described above, I generated a sparse-coded version
of the TIMIT dataset using my gammatone sparse coding library. I used
gammatones with 50 different cosine frequencies between 50 and 8000
Hz, timeshifts of 8 samples, and frames of length 160 with 50%
overlap. For each frame, a sparse representation using 16 non-zero
coefficients was extracted by using Orthogonal Matching Pursuit with a
sparsity constraint.&lt;/p&gt;
&lt;p&gt;This data and the one-hot encoded information about the previous,
current, and next phone were used to train an MLP with the following
characteristics:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;Two rectified linear hidden layers (2150 and 950 units, respectively);&lt;/li&gt;
&lt;li&gt;Linear output layer with 950 units (one for each sparse coding coefficient);&lt;/li&gt;
&lt;li&gt;Training: batch gradient descent (batch size of 512 samples), with squared error objective;&lt;/li&gt;
&lt;li&gt;Termination criteria: 10 epochs with objective decrease lower than
&lt;span class="math"&gt;\(10^{-6}\)&lt;/span&gt;
 or 200 epochs.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;However, something strange happened when I tried to train this
network: it has converged after 10 epochs! Of course this would be too
good to be true, which means something terrible happened instead. In
my case, the training, testing, and validation objectives did not
change at all with training iterations. I still do not know exactly
what happened, but I suspect the large amount of zeros in the input
and target values made the majority of the gradients equal to zero,
and without gradients none of the weights will change. Maybe a
different kind of initialization could solve this issue, but there are
other problems as well. Namely, this network does nothing to enforce
sparsity at the output, and in the end the output coefficients will
have a distribution that is very different from the target
coefficients (which are zero most of the time). Prof. Bengio suggested
that I could try making the output distribution the product of a
Bernoulli distribution and a Gaussian distribution: the first one
would say if that coefficient should be zero or not, and the latter
would give its value. However, he noted that this is just an arbitrary
statistical model which probably does not correspond to the real
behavior of the coefficients, and we would probably be better by
trying to estimate this distribution too (maybe with an RBM).&lt;/p&gt;
&lt;p&gt;While trying to solve these issues, I had an idea for another
architecture that could be easier to implement...&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="splitting-signal-into-spectral-envelope-and-phase"&gt;
&lt;h2&gt;Splitting signal into spectral envelope and phase&lt;/h2&gt;
&lt;p&gt;As &lt;a class="reference external" href="http://ift6266speechsynthesisjt.wordpress.com/2014/03/19/randomized-phases-preserves-speech-content-and-identity/"&gt;Jessica&lt;/a&gt; pointed out in her blog, most of the relevant
information in a speech signal is encoded in its envelope. Because of
that, we are less sensitive to phase distortions than to envelope
distortions. As we have already discussed in class, as speech envelope
variations are slower than the phase variations, some speech coding
models (such as LPC) take these facts into account by encoding the
envelope and the phase separately (and usually using a simpler model
for the phase than for envelopes).&lt;/p&gt;
&lt;p&gt;It was also brought to my attention that a recent paper &lt;a class="citation-reference" href="#han2014" id="id3"&gt;[Han2014]&lt;/a&gt; to
be presented at this year's ICASSP uses gammatone filterbank features
to find spectral masks to use in speech dereverberation. The advantage
of using gammatone filterbanks instead of a simple STFT is that with
the gammatone filterbank, we are able to fine-tune spectral resolution
at lower frequency bands (the most important band for speech
content). While speech dereverberation is a totally different topic,
the feature space used in that paper is still relevant. They are
looking for spectral masks to filter an existing signal and not on
synthesis, so they can discard the phase completely. For our project,
we cannot do that but we could work with a slightly different
approach.&lt;/p&gt;
&lt;p&gt;We have one network that is trained on spectral envelopes, using a
similar approach to that of the paper. This network is trained using
the gammatonegram, which consists of the total gammatone band energy
in all channels of our filterbank per frame. The figure below depicts
how this is done:&lt;/p&gt;
&lt;img alt="" class="align-center" src="images/filterbank.png" style="width: 700px;" /&gt;
&lt;p&gt;Here, &lt;span class="math"&gt;\(y_i[n], i = 1, \dots, 64\)&lt;/span&gt;
 are the frame energies (sum of
squared samples) for each gammatone channel (I'm using 64 channels
here as this is what was used in &lt;a class="citation-reference" href="#han2014" id="id4"&gt;[Han2014]&lt;/a&gt; and can be a good starting
point). As inputs of this network, we would use the gammatonegram of a
number of previous frames, one-hot encoded phones for these frames
(and possibly some of the next frames), and the output would be the
gammatonegram of the next frame. This is not enough to resynthesize a
speech signal as we don't have the phase, but that could be solved by
training a separate model for phases, either for an overall phase or a
per-channel phase. Resynthesis is done according to the following
signal flow diagram:&lt;/p&gt;
&lt;img alt="" class="align-center" src="images/synthesis.png" style="width: 500px;" /&gt;
&lt;p&gt;Here, &lt;span class="math"&gt;\(p_i[n], i=1, \dots, 64\)&lt;/span&gt;
 are vectors representing the
phase of each channel and &lt;span class="math"&gt;\(g_i[n], i=1, \dots, 64\)&lt;/span&gt;
 are the
amplitudes for each gammatone channel (which could be either the
&lt;span class="math"&gt;\(y_i[n]\)&lt;/span&gt;
 values computed before for each frame or a smoothed
version of them).&lt;/p&gt;
&lt;p&gt;For the network architecture, I am planning on using an MLP (possibly
with unsupervised pretraining) for the spectral envelopes. For the
phase components, I will initially try RBMs using previous phase
samples, phone codes, and speaker characteristics (pitch, gender,
etc.) as input. I expect to be able to use simpler models for the
phase (or at least be able to control this model's complexity, as I
believe there should be a tradeoff between speech quality and the
accuracy of the phase models). I have already extracted the gammatone
features from the whole database and will report results for the
spectral envelope model on my next post.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="references"&gt;
&lt;h2&gt;References&lt;/h2&gt;
&lt;table class="docutils citation" frame="void" id="smith2006" rules="none"&gt;
&lt;colgroup&gt;&lt;col class="label" /&gt;&lt;col /&gt;&lt;/colgroup&gt;
&lt;tbody valign="top"&gt;
&lt;tr&gt;&lt;td class="label"&gt;&lt;a class="fn-backref" href="#id1"&gt;[Smith2006]&lt;/a&gt;&lt;/td&gt;&lt;td&gt;E. C. Smith and M. S. Lewicki, “Efficient auditory coding,” Nature, vol. 439, no. 7079, pp. 978–982, 2006.&lt;/td&gt;&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;table class="docutils citation" frame="void" id="strahl2008" rules="none"&gt;
&lt;colgroup&gt;&lt;col class="label" /&gt;&lt;col /&gt;&lt;/colgroup&gt;
&lt;tbody valign="top"&gt;
&lt;tr&gt;&lt;td class="label"&gt;&lt;a class="fn-backref" href="#id2"&gt;[Strahl2008]&lt;/a&gt;&lt;/td&gt;&lt;td&gt;S. Strahl and A. Mertins, “Sparse gammatone signal model optimized for English speech does not match the human auditory filters,” Brain research, vol. 1220, pp. 224–233, 2008.&lt;/td&gt;&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;table class="docutils citation" frame="void" id="han2014" rules="none"&gt;
&lt;colgroup&gt;&lt;col class="label" /&gt;&lt;col /&gt;&lt;/colgroup&gt;
&lt;tbody valign="top"&gt;
&lt;tr&gt;&lt;td class="label"&gt;[Han2014]&lt;/td&gt;&lt;td&gt;&lt;em&gt;(&lt;a class="fn-backref" href="#id3"&gt;1&lt;/a&gt;, &lt;a class="fn-backref" href="#id4"&gt;2&lt;/a&gt;)&lt;/em&gt; K. Han, Y. Wang, D. Wang, “Learning spectral mapping for speech dereverberation”, To appear in the Proceedings of the IEEE ICASSP 2014, 2014. Available at &lt;a class="reference external" href="http://www.cse.ohio-state.edu/~dwang/papers/HWW.icassp14.pdf"&gt;http://www.cse.ohio-state.edu/~dwang/papers/HWW.icassp14.pdf&lt;/a&gt;.&lt;/td&gt;&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
</summary><category term="ift6266"></category></entry><entry><title>Experiments with a 2 layer MLP incorporating phone information</title><link href="/exp_mlp.html" rel="alternate"></link><updated>2014-03-19T21:00:00-04:00</updated><author><name>jfsantos</name></author><id>tag:,2014-03-19:exp_mlp.html</id><summary type="html">&lt;p&gt;During the spring break I decided to run some experiments with MLPs as
generative models, using both acoustic samples and phone codes as input.
The experiment's objective is two-fold: I wanted to investigate if using
information from surrounding frames improves the models (when compared
to what our colleagues have found), and I also wanted to have a baseline
to compare to the models based on sparse coding that I have been working
on. In the experiments by
&lt;a class="reference external" href="http://ift6266hjb.wordpress.com/2014/02/10/speech-synthesis-project-description-and-first-attempt-at-a-regression-mlp/"&gt;Hubert&lt;/a&gt;,
&lt;a class="reference external" href="http://jpraymond.wordpress.com/2014/02/27/results-with-a-one-hidden-layer-neural-net/"&gt;Jean-Phillipe&lt;/a&gt;,
and &lt;a class="reference external" href="http://twuilliam.wordpress.com/2014/02/27/quick-experiment-breaking-the-sin-in-one-line/"&gt;William&lt;/a&gt;, only the acoustic samples information was used as
input.
&lt;a class="reference external" href="http://amjadmahayri.wordpress.com/2014/02/27/frame-prediction-given-phoneme-window/"&gt;Amjad&lt;/a&gt;
has already done some tests incorporating phone information, but it
seems he is using only the current phone.&lt;/p&gt;
&lt;p&gt;In my experiments, I updated
&lt;a class="reference external" href="http://vdumoulin.github.io/articles/timit-part-5"&gt;Vincent's&lt;/a&gt; dataset
implementation in order to make it provide the phones corresponding to
the current, previous, and next frame. The code can be found in my
&lt;a class="reference external" href="https://github.com/jfsantos/research"&gt;fork&lt;/a&gt;. Previously I was using
Python code to setup pylearn2 experiments, but I decided to switch to
YAML for these experiments as I didn't need to do anything fancy. The
YAML for this experiment can be found
&lt;a class="reference external" href="https://github.com/jfsantos/ift6266h14/blob/master/experiments/mlp_acoustic/ac160_ph3_rl2_malespkr.yaml"&gt;here&lt;/a&gt; and the serialized model is &lt;a class="reference external" href="https://github.com/jfsantos/ift6266h14/blob/master/experiments/mlp_acoustic/ac160_ph3_rl2_malespkr.pkl"&gt;here&lt;/a&gt;.
The dataset was configured as follows:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;Frame length: 160 samples&lt;/li&gt;
&lt;li&gt;Frame overlap: 0 (not ideal, but it can be seen as a subsampling of
the complete dataset)&lt;/li&gt;
&lt;li&gt;Frames per example: 1&lt;/li&gt;
&lt;li&gt;Number of predicted samples: 1&lt;/li&gt;
&lt;li&gt;Phone information: one-hot encoded phone code for the previous,
current, and next frame&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;With this configuration, each example is a vector with
&lt;span class="math"&gt;\(160 + 3*62 = 346\)&lt;/span&gt;
 values. The MLP was set-up and trained as
follows:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;Two rectified linear hidden layers (the first with 500 and the second
with 100 units)&lt;/li&gt;
&lt;li&gt;Linear output layer with a single unit (a single sample is predicted
for each input)&lt;/li&gt;
&lt;li&gt;Training algorithm: SGD with fixed learning rate of 0.01, running for
a maximum of 200 epochs (alternative convergence condition was set as
10 iterations with improvement lower than &lt;span class="math"&gt;\(1^{-10}\)&lt;/span&gt;
). The batch
size was of 512 examples.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Total training time for this experiment was approximately 1.15 hours,
running on a CPU (Intel Core i7-2600, 8 GB RAM, with Theano running over
MKL and using 4 cores simultaneously). As mentioned before, I considered
10 iterations without improvement as the convergence condition, which
happened by the iteration 159. A plot for the training, testing, and
validation set errors can be seen below. The errors found after
convergence (for the normalized, i.e., centered and divided by the
standard deviation) were the following:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;Training error: 0.02284&lt;/li&gt;
&lt;li&gt;Test error: 0.03309&lt;/li&gt;
&lt;li&gt;Validation error: 0.05482&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;A plot showing the evolution of the errors over epochs can be seen below.&lt;/p&gt;
&lt;img alt="" src="images/exp_mlp_1.png" /&gt;
&lt;p&gt;To evaluate the trained network as a synthesizer, I got a sequence of
phone codes straight out of a sentence in the validation set and used it
as input to the MLP. As I did not have a previous frame, the initial
input is a frame with only zeros on it. I played with a multiplicative
factor on the noise added to the Gaussian sampling as &lt;a class="reference external" href="http://davidtob.wordpress.com/2014/03/14/generating-one-phone-from-one-timit-speaker/"&gt;David
did&lt;/a&gt;,
as using directly the test error I ended up with bursts as can be seen
below. The following multiplicative factors were tested:
&lt;tt class="docutils literal"&gt;[0.01, 0.05, 0.1, 0.5, 1, 2, 5, 10]&lt;/tt&gt;. As using high noise levels ends
up corrupting too much the signal, I filtered them down to approximately
the telephone bandwidth (300-4000 Hz) with a &lt;span class="math"&gt;\(4^{th}\)&lt;/span&gt;
 order
Butterworth passband filter, just to reduce the overall effect of noisy
sampling. For low noise multipliers, all I got was a short burst and
then the output stays at zero. However, by increasing the noise level to
five times the mean square test error, apparently I got some more
structure: even though it has almost nothing to do with whatever should
have been synthesized, it does sound like multiple speakers babbling. The respective audio files and plots (acoustic waveform + spectrogram) can be seen below:&lt;/p&gt;
&lt;p&gt; 0.01: &lt;br&gt;
&lt;audio controls="controls" &gt;
      &lt;source src="files/y_noise_0.01.ogg" type="audio/wav" /&gt;
      Your browser does not support the audio element.
&lt;/audio&gt; &lt;/p&gt;
&lt;p&gt; 0.05: &lt;br&gt;
&lt;audio controls="controls" &gt;
      &lt;source src="files/y_noise_0.05.ogg" type="audio/wav" /&gt;
      Your browser does not support the audio element.
&lt;/audio&gt; &lt;/p&gt;
&lt;p&gt; 0.1: &lt;br&gt;
&lt;audio controls="controls" &gt;
      &lt;source src="files/y_noise_0.1.ogg" type="audio/wav" /&gt;
      Your browser does not support the audio element.
&lt;/audio&gt; &lt;/p&gt;
&lt;p&gt; 0.5: &lt;br&gt;
&lt;audio controls="controls" &gt;
      &lt;source src="files/y_noise_0.5.ogg" type="audio/wav" /&gt;
      Your browser does not support the audio element.
&lt;/audio&gt; &lt;/p&gt;
&lt;p&gt; 1.0: &lt;br&gt;
&lt;audio controls="controls" &gt;
      &lt;source src="files/y_noise_1.ogg" type="audio/wav" /&gt;
      Your browser does not support the audio element.
&lt;/audio&gt; &lt;/p&gt;
&lt;p&gt; 2.0: &lt;br&gt;
&lt;audio controls="controls" &gt;
      &lt;source src="files/y_noise_2.ogg" type="audio/wav" /&gt;
      Your browser does not support the audio element.
&lt;/audio&gt; &lt;/p&gt;
&lt;p&gt; 5.0: &lt;br&gt;
&lt;audio controls="controls" &gt;
      &lt;source src="files/y_noise_5.ogg" type="audio/wav" /&gt;
      Your browser does not support the audio element.
&lt;/audio&gt; &lt;/p&gt;
&lt;p&gt; 10.0: &lt;br&gt;
&lt;audio controls="controls" &gt;
      &lt;source src="files/y_noise_10.ogg" type="audio/wav" /&gt;
      Your browser does not support the audio element.
&lt;/audio&gt; &lt;/p&gt;&lt;img alt="" src="images/exp_mlp_2.png" /&gt;
&lt;p&gt;One interesting thing: I did the same procedure to generate an output
during training, sometime around the &lt;span class="math"&gt;\(130^{th}\)&lt;/span&gt;
 iteration. The
output generated at that stage sounded much nicer than what I got
after the training finished, but unfortunately the pickled model was
overwritten because of the way I set up my YAML file, which overwrites
the old model every time an iteration improves the objective. The only
thing I kept was the output:&lt;/p&gt;
&lt;audio controls="controls" &gt;
  &lt;source controls src="files/malespkr_rl2_not_converged.ogg"&gt; type="audio/ogg" /&gt;
  Your browser does not support the audio element.
&lt;/audio&gt;&lt;div class="section" id="next-steps"&gt;
&lt;h2&gt;Next steps&lt;/h2&gt;
&lt;p&gt;Moving forward, I will write in my next post about the (not so
successful) tests I did using sparse coding coefficients instead of
acoustic samples as inputs. I will also comment about some ideas to
incorporate more advanced models in my experiments.&lt;/p&gt;
&lt;/div&gt;
</summary><category term="ift6266"></category></entry><entry><title>Dictionary Learning and Sparse Coding for Speech Signals</title><link href="/dictlearning.html" rel="alternate"></link><updated>2014-02-25T17:00:00-05:00</updated><author><name>jfsantos</name></author><id>tag:,2014-02-25:dictlearning.html</id><summary type="html">&lt;p&gt;Sparse signal approximations are the basis for a variety of signal
processing techniques. Such approximations are usually employed with
the objective of having a signal representation that is more
meaningful, malleable, and robust to noise than the ones obtained by
standard transform methods &lt;a class="citation-reference" href="#sturm2009" id="id1"&gt;[Sturm2009]&lt;/a&gt;. The so-called dictionary
based methods (DBM) decompose a signal into a linear combination of
waveforms through an approximation technique such as Matching Pursuit
(MP) &lt;a class="citation-reference" href="#mallat1993" id="id2"&gt;[Mallat1993]&lt;/a&gt;, Orthogonal Matching Pursuit (OMP) &lt;a class="citation-reference" href="#pati1993" id="id3"&gt;[Pati1993]&lt;/a&gt;, or
basis pursuit &lt;a class="citation-reference" href="#chen2001" id="id4"&gt;[Chen2001]&lt;/a&gt;. The collection of waveforms that can be
selected for the linear combination is called a dictionary. This
dictionary is usually overcomplete, either because it is formed by
merging complete dictionaries or because the waveforms are chosen
arbitrarily (and we have more waveforms than the length of the signal
we want to represent).&lt;/p&gt;
&lt;div class="section" id="sparse-approximation-problem-formulations"&gt;
&lt;h2&gt;Sparse approximation problem formulations&lt;/h2&gt;
&lt;p&gt;The sparse coding problem is usually formulated either as a
sparsity-constrained problem or as an error-constrained problem. The
formulations are as follows:&lt;/p&gt;
&lt;p&gt;Sparsity-constrained:
&lt;span class="math"&gt;\(\underline{\hat{\gamma}} = \underset{\underline{\gamma}}{arg\,min}\|\underline{x} - D \underline{\gamma}\|_2^2 \quad\text{s.t.}\quad   \|\underline{\gamma}\|_0 \leq K\)&lt;/span&gt;
&lt;/p&gt;
&lt;p&gt;Error-constrained:
&lt;span class="math"&gt;\(\underline{\hat{\gamma}} = \underset{\underline{\gamma}}{arg\,min}\|\underline{\gamma}\|_0 \quad\text{s.t.}\quad \|\underline{x} - D \underline{\gamma}\|_2^2 \leq \epsilon\)&lt;/span&gt;
&lt;/p&gt;
&lt;p&gt;In the first one, the idea is that we want to represent the signal by
a linear combination of up to K known waveforms. In the second
formulation, we want the squared error of the representation to be
below a certain threshold. Both formulations are useful, depending on
the problem you are trying to solve: the first one will lead to more
compact representations, while with the second one you can avoid
higher representation errors.&lt;/p&gt;
&lt;p&gt;The second formulation is also useful for applications which need
denoising: consider you have a corrupted version of your signal, and
also that you know (more or less) the signal-to-noise ratio (SNR). If
the noise is very different from the signal you are interested in and
your dictionary is optimized to represent these signals only, it may
be the case that noise is not well represented by the waveforms in
your dictionary. So, you could use the second formulation, setting
&lt;span class="math"&gt;\(\epsilon\)&lt;/span&gt;
 as the estimated noise level, and expect that a good
part of the noise component is not going to be represented in the
sparse approximation.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="dictionary-learning"&gt;
&lt;h2&gt;Dictionary learning&lt;/h2&gt;
&lt;p&gt;Reconstructing a speech signal based on a learned set of segments is not
a new thing. It is done in a well-known technique called vector
quantization (VQ). In VQ, the signal is reconstructed by using only a
single atom (or &lt;em&gt;codeword&lt;/em&gt;, on the VQ literature jargon) per signal
frame. The dictionary (or &lt;em&gt;codebook&lt;/em&gt;) is usually designed by a
nearest-neighbor method, which aims to find the codebook that can
reconstruct a signal by using the codewords that have the smaller
distances to the original signal frames while minimizing the residual.
K-means is a codebook learning algorithm for VQ that solves this problem
by dividing the training samples into &lt;span class="math"&gt;\(K\)&lt;/span&gt;
 clusters of the nearest
neighbors of each of the &lt;span class="math"&gt;\(K\)&lt;/span&gt;
 items in the initial codebook. The
codebook is then updated by finding the centroid for each of the
&lt;span class="math"&gt;\(K\)&lt;/span&gt;
 clusters. These steps are ran iteratively until the algorithm
converges to a local minimum solution.&lt;/p&gt;
&lt;p&gt;For sparse coding, we want to use multiple atoms to reconstruct the
signal. In the snippet below, we generate a dictionary with 1024
waveforms by using the dictionary learning functions available in
&lt;a class="reference external" href="http://scikit-learn.org"&gt;scikit-learn&lt;/a&gt;, which is based on a paper by &lt;a class="citation-reference" href="#mairal2009" id="id5"&gt;[Mairal2009]&lt;/a&gt;. The
training data consists of two minutes of audio from the TIMIT
database; sentences were randomly chosen and then split into frames of
256 samples each.&lt;/p&gt;
&lt;pre class="code python literal-block"&gt;
&lt;span class="c"&gt;# Build the dictionary&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;sklearn.decomposition&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;MiniBatchDictionaryLearning&lt;/span&gt;
&lt;span class="n"&gt;dico&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;MiniBatchDictionaryLearning&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n_components&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1024&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;alpha&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;n_iter&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;100&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;D&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;dico&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;training_data&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;components_&lt;/span&gt;
&lt;/pre&gt;
&lt;img alt="" class="align-center" src="images/dictlearning_5_1.png" style="width: 720px;" /&gt;
&lt;p&gt;If we take a look into some of the learned waveforms in the figure
above, we'll see that we have both low-frequency, quasiperiodic
signals (which are probably matching vowels) and signals with more
high-frequency components that look a bit noisy (probably representing
stops/fricatives).&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="reconstructing-speech-segments-using-sparse-coding-with-the-learned-dictionary"&gt;
&lt;h2&gt;Reconstructing speech segments using sparse coding with the learned dictionary&lt;/h2&gt;
&lt;p&gt;Now that we have a dictionary which (supposedly) is good for
representing speech signals, let's use Orthogonal Matching Pursuit
(OMP) to reconstruct a speech segment based on a linear combination of
dictionary entries. Let's get 10 seconds of audio from TIMIT (from a
segment of the set that was not in the training set) and reconstruct
it using a sparse approximation. We use the sparsity-based constraint
form, as we are more interested in representing speech in a sparse
way:&lt;/p&gt;
&lt;pre class="code python literal-block"&gt;
&lt;span class="c"&gt;# Get sample speech segment to reconstruct&lt;/span&gt;
&lt;span class="n"&gt;test_data&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;fs&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="mi"&gt;200&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="n"&gt;fs&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="mi"&gt;210&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reshape&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;fs&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="mi"&gt;256&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;256&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c"&gt;# Reconstruct it frame-by-frame using a linear combination of 20&lt;/span&gt;
&lt;span class="c"&gt;# atoms per frame (sparsity-constrained OMP)&lt;/span&gt;
&lt;span class="n"&gt;result&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;numpy&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ndarray&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="n"&gt;test_data&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;&lt;span class="mi"&gt;512&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;

&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;sklearn.decomposition&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;SparseCoder&lt;/span&gt;

&lt;span class="n"&gt;coder&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;SparseCoder&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;dictionary&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;D&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;transform_n_nonzero_coefs&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;20&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                    &lt;span class="n"&gt;transform_alpha&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;None&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;transform_algorithm&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;omp&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;result&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;coder&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;transform&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;test_data&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;n&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;result&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]):&lt;/span&gt;
    &lt;span class="n"&gt;out&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="mi"&gt;256&lt;/span&gt;&lt;span class="p"&gt;:(&lt;/span&gt;&lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="mi"&gt;256&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;D&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;T&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;result&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;&lt;span class="n"&gt;axis&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;
&lt;p&gt;Here are the results: you can listen above the original file and the reconstructed one.&lt;/p&gt;
&lt;p&gt; Original: &lt;br&gt;
&lt;audio controls="controls" &gt;
      &lt;source src="files/orig.ogg" type="audio/wav" /&gt;
      Your browser does not support the audio element.
&lt;/audio&gt; &lt;/p&gt;
&lt;p&gt; Reconstructed with 20 atoms/frame:&lt;br&gt;
&lt;audio controls="controls" &gt;
      &lt;source src="files/reconst.ogg" type="audio/wav" /&gt;
      Your browser does not support the audio element.
&lt;/audio&gt;&lt;/p&gt;&lt;p&gt;These figures show the original signal, the reconstructed one, and the squared error:&lt;/p&gt;
&lt;img alt="" class="align-center" src="images/dictlearning_10_1.png" /&gt;
&lt;p&gt;While the reconstruction error is low for most of the time considering
we are using only 20 non-zero values per frame to represent the
signal, as opposed to using 256 samples, we can clearly hear the
reconstruction-related artifacts. However, that may be OK if all we
want with the learned dictionary is to have a sparser representation
for speech that will be used later in our synthesizer.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="relationship-with-our-project-and-next-steps"&gt;
&lt;h2&gt;Relationship with our project and next steps&lt;/h2&gt;
&lt;p&gt;I started working on some experiments comparing the performance of a
sample predictor to two other predictors: one based on LPC
coefficients and the other on a sparse representation of speech. As we
discussed in class, speech has some parameters that change quickly
(source/excitation signal), while others change slowly
(articulation-related). In the first experiments prof. Bengio
suggested, we were working on an MLP-based generative model for
samples without any consideration for phones. His second suggestion
was to design a generative model for the next sample conditioned on
the previous, current, and next phone.&lt;/p&gt;
&lt;p&gt;I started developing generative models based on MLPs for the three
representations above, using one-hot encoded phones and the relative
position in time of the current phone as inputs. For the model based
on LPCs, I am planning to have a separate generative model for the
excitation signal, which is going to work pretty much like the
next-sample predictor we worked on previously; this model could also
be based on the previous, current, and next phone, previous samples,
and things such as pitch/speaker gender. Unfortunately, due to a &lt;a class="reference external" href="https://groups.google.com/forum/#!topic/pylearn-users/EZ3H8xP7gN8"&gt;bug&lt;/a&gt;
in pylearn2 I was not able to get them working yet. &lt;a class="reference external" href="http://vdumoulin.github.io/"&gt;Vincent&lt;/a&gt; said
there's already a &lt;a class="reference external" href="https://github.com/lisa-lab/pylearn2/pull/512"&gt;pull request&lt;/a&gt; which solves this
issue and it seems it will get fixed anytime soon.&lt;/p&gt;
&lt;p&gt;Last note: you can view the IPython notebook containing all the code used to generate the dictionary and the plots &lt;a class="reference external" href="http://nbviewer.ipython.org/urls/seaandsailor.com/files/dictlearning.ipynb"&gt;here&lt;/a&gt;, or &lt;a class="reference external" href="files/dictlearning.ipynb"&gt;download&lt;/a&gt; and run it interactively in your computer.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="references"&gt;
&lt;h2&gt;References&lt;/h2&gt;
&lt;table class="docutils citation" frame="void" id="sturm2009" rules="none"&gt;
&lt;colgroup&gt;&lt;col class="label" /&gt;&lt;col /&gt;&lt;/colgroup&gt;
&lt;tbody valign="top"&gt;
&lt;tr&gt;&lt;td class="label"&gt;&lt;a class="fn-backref" href="#id1"&gt;[Sturm2009]&lt;/a&gt;&lt;/td&gt;&lt;td&gt;B. L. Sturm, C. Roads, A. McLeran, and J. J. Shynk, “Analysis, Visualization, and Transformation of Audio Signals Using Dictionary-based Methods†,” Journal of New Music Research, vol. 38, no. 4, pp. 325–341, 2009.&lt;/td&gt;&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;table class="docutils citation" frame="void" id="mallat1993" rules="none"&gt;
&lt;colgroup&gt;&lt;col class="label" /&gt;&lt;col /&gt;&lt;/colgroup&gt;
&lt;tbody valign="top"&gt;
&lt;tr&gt;&lt;td class="label"&gt;&lt;a class="fn-backref" href="#id2"&gt;[Mallat1993]&lt;/a&gt;&lt;/td&gt;&lt;td&gt;S. G. Mallat and Z. Zhang, “Matching pursuits with time-frequency dictionaries,” IEEE Transactions on Signal Processing, vol. 41, no. 12, pp. 3397–3415, Dec. 1993.&lt;/td&gt;&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;table class="docutils citation" frame="void" id="pati1993" rules="none"&gt;
&lt;colgroup&gt;&lt;col class="label" /&gt;&lt;col /&gt;&lt;/colgroup&gt;
&lt;tbody valign="top"&gt;
&lt;tr&gt;&lt;td class="label"&gt;&lt;a class="fn-backref" href="#id3"&gt;[Pati1993]&lt;/a&gt;&lt;/td&gt;&lt;td&gt;Y. C. Pati, R. Rezaiifar, and P. S. Krishnaprasad, “Orthogonal matching pursuit: Recursive function approximation with applications to wavelet decomposition,” in Signals, Systems and Computers, 1993. 1993 Conference Record of The Twenty-Seventh Asilomar Conference on, 1993, pp. 40–44.&lt;/td&gt;&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;table class="docutils citation" frame="void" id="chen2001" rules="none"&gt;
&lt;colgroup&gt;&lt;col class="label" /&gt;&lt;col /&gt;&lt;/colgroup&gt;
&lt;tbody valign="top"&gt;
&lt;tr&gt;&lt;td class="label"&gt;&lt;a class="fn-backref" href="#id4"&gt;[Chen2001]&lt;/a&gt;&lt;/td&gt;&lt;td&gt;S. S. Chen, D. L. Donoho, and M. A. Saunders, “Atomic decomposition by basis pursuit,” SIAM journal on scientific computing, vol. 20, no. 1, pp. 33–61, 1998.&lt;/td&gt;&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;table class="docutils citation" frame="void" id="mairal2009" rules="none"&gt;
&lt;colgroup&gt;&lt;col class="label" /&gt;&lt;col /&gt;&lt;/colgroup&gt;
&lt;tbody valign="top"&gt;
&lt;tr&gt;&lt;td class="label"&gt;&lt;a class="fn-backref" href="#id5"&gt;[Mairal2009]&lt;/a&gt;&lt;/td&gt;&lt;td&gt;J. Mairal, F. Bach, J. Ponce, and G. Sapiro, “Online dictionary learning for sparse coding,” in Proceedings of the 26th Annual International Conference on Machine Learning, 2009, pp. 689–696.&lt;/td&gt;&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
</summary><category term="ift6266"></category></entry><entry><title>Speech signal representations</title><link href="/initial_representation.html" rel="alternate"></link><updated>2014-02-01T18:00:00-05:00</updated><author><name>jfsantos</name></author><id>tag:,2014-02-01:initial_representation.html</id><summary type="html">&lt;p&gt;One of the objectives of our project is to learn a useful
representation from speech signals that can be used to synthesize new
(arbitrary) sentences. There are many different ways of representing
speech signals; those representations are usually tailored to specific
applications. In speech recognition, for example, we want to minimize
the variability from different speakers while keeping sufficient
information to discriminate different phonemes. In speech coding,
however, we usually want to keep information that is associated with
the speaker's identity as well as reduce the amount of data to be
stored/transmitted.&lt;/p&gt;
&lt;p&gt;Our dataset was initially distributed as frame MFCCs (input) and
one-hot encoded phonemes (labels). While this representation is
usually enough for speech recognition, I believe it is not enough for
learning a useful representation for synthesis (as briefly mentioned
by Laurent Dinh in his &lt;a class="reference external" href="http://deeprandommumbling.wordpress.com/2014/01/29/listening-to-a-vector"&gt;post&lt;/a&gt;). The reason is that MFCCs are a
destructive/lossy representation of a speech signal. First,
fundamental frequency information is completely lost, as well as
instantaneous phase. MFCCs more or less represent the energy in
different frequency channels that are considered important for human
speech (following the Mel scale &lt;a class="citation-reference" href="#stevens2005" id="id1"&gt;[Stevens2005]&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;In this post, I will present some alternative speech signal
representations that may be more suitable for speech synthesis. Even
though one of our objectives is to learn a representation, we need to
understand a little bit about what has been developed by the speech
processing community, as it can serve as an inspiration.&lt;/p&gt;
&lt;div class="section" id="acoustic-samples-time-domain"&gt;
&lt;h2&gt;Acoustic samples (time domain)&lt;/h2&gt;
&lt;p&gt;Using raw acoustic samples from overlapping frames is the simplest
approach. A discrete signal &lt;span class="math"&gt;\(x[n]\)&lt;/span&gt;
 is simply a sequence of (real
or integer) numbers corresponding to the signal samples (sampled
uniformly at an arbitrary sampling rate). The usual sampling rate for
speech recognition applications is 16 kHz, while the sampling rate
used for &amp;quot;telephone speech&amp;quot; coding is 8 kHz. This is essentially the
information we find in a PCM-encoded WAV file.&lt;/p&gt;
&lt;!-- add plots --&gt;
&lt;div class="figure align-center"&gt;
&lt;img alt="" src="images/timedomain.png" /&gt;
&lt;p class="caption"&gt;&lt;em&gt;Time-domain speech signal sampled at 16 kHz.&lt;/em&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class="figure align-center"&gt;
&lt;img alt="" src="images/timedomain_zoom.png" /&gt;
&lt;p class="caption"&gt;&lt;em&gt;Zoom of a 200-sample segment of the above signal.&lt;/em&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="section" id="short-time-fourier-transform"&gt;
&lt;h2&gt;Short-time Fourier Transform&lt;/h2&gt;
&lt;p&gt;Another possible representation is to use Short-time Fourier Transform
(STFT) coefficients from overlapping frames. This is essentially the
same as using raw acoustic samples in the sense that there is no
information loss, but the representation in the frequency domain is
usually clearer for humans because we can associate the content in
different frequency bands with different phonemes. The STFT of a
discrete signal &lt;span class="math"&gt;\(x[n]\)&lt;/span&gt;
 is given by:&lt;/p&gt;
&lt;div class="math"&gt;
\begin{equation*}
STFT{x[n]}(m,\omega) = X(m,\omega) = \sum_{n=-\infty}^{\infty} x[n] w[n-m] e^{-j \omega n}
\end{equation*}
&lt;/div&gt;
&lt;p&gt;where &lt;span class="math"&gt;\(n,\omega\)&lt;/span&gt;
 are the time and frequency indexes, and
&lt;span class="math"&gt;\(w[n]\)&lt;/span&gt;
 is the windowing function. A spectrogram is the
magnitude-squared version of this equation (i.e., without phase
information).&lt;/p&gt;
&lt;p&gt;Spectrograms can be done using windows with different lengths. This is
related to the &lt;a class="reference external" href="http://en.wikipedia.org/wiki/Short-time_Fourier_transform#Resolution_issues"&gt;Gabor (or Heisenberg-Gabor)&lt;/a&gt; limit: we cannot
simultaneously localize a signal in both time and frequency domains
with a high degree of certainty. Therefore, we usually have to use
different window lengths depending on what we want to analyze: wide
windows have better frequency resolution and bad time resolution,
while the opposite happens for short windows. A possible compromise is
to choose a single window length that has sufficient resolution for
the target application.&lt;/p&gt;
&lt;div class="figure align-center"&gt;
&lt;img alt="" src="images/specgram.png" /&gt;
&lt;p class="caption"&gt;&lt;em&gt;Spectrogram (using a 20 ms rectangular window) of the speech signal above.&lt;/em&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="section" id="linear-predictive-coding"&gt;
&lt;h2&gt;Linear Predictive Coding&lt;/h2&gt;
&lt;p&gt;Linear Predictive Coding &lt;a class="citation-reference" href="#o1988linear" id="id2"&gt;[o1988linear]&lt;/a&gt; coefficients + residual
(basically excitation information). LPC is based on the source-filter
model of speech production, which assumes a speech signal is produced
by filtering a series of pulses (and eventually noise bursts). The LPC
coefficients are related to the position of the articulators in the
mouth (e.g., tongue, palate, lips), while the pitch/noise information
is related to how the vocal tract is excited. This is usually
represented as an auto-regressive (AR) model with order &lt;span class="math"&gt;\(p\)&lt;/span&gt;
:&lt;/p&gt;
&lt;div class="math"&gt;
\begin{equation*}
x[n] = \sum_{k=1}^{p} a_k x[n-k]
\end{equation*}
&lt;/div&gt;
&lt;p&gt;where &lt;span class="math"&gt;\(a[k]\)&lt;/span&gt;
 are the model's coefficients. LPCs are computed for each speech frame based on a least-squares method:&lt;/p&gt;
&lt;div class="math"&gt;
\begin{equation*}
\arg\min_{a_k} \sum_{-\infty}^\infty [x[n] - \sum_{k=1}^p a_k x[n-k]]^2
\end{equation*}
&lt;/div&gt;
&lt;p&gt;Because of its error criteria, LPC also has problems to represent the
phase of acoustic signals (by squaring the error, we are modeling the
spectral magnitude of the signal, and not the phase). For this reason,
LPC speech may sound artificial when resynthesized. More robust
methods are used nowadays, such as the code-excited linear prediction
(CELP) &lt;a class="citation-reference" href="#valin2006speex" id="id3"&gt;[valin2006speex]&lt;/a&gt;. These methods, for example, use
psychoacoustics-inspired techniques to shape the coding noise to
frequency regions where the human auditory system is more tolerant. In
CELP, the residual is not transmitted directly, but represented as
entries in two codebooks.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="wavelets"&gt;
&lt;h2&gt;Wavelets&lt;/h2&gt;
&lt;p&gt;Them main purpose of a wavelet transform is to decompose arbitrary
signals into localized contributions that can be labelled by a scale
(or resolution) parameter &lt;a class="citation-reference" href="#mallat1989theory" id="id4"&gt;[mallat1989theory]&lt;/a&gt;. The representation
achieved through the wavelet transform can be seen as hierarchical: at
a coarse resolution, we have an idea of “context”, while with highest
resolution we can see more details. This is achieved by decomposing
the original signal using a set of functions well-localized both in
time and frequency (the so-called wavelets).&lt;/p&gt;
&lt;p&gt;Discrete wavelet transforms are implemented as a cascade of digital
filters with transfer functions derived from a discrete &amp;quot;mother
wavelet&amp;quot;. The figure below shows an example. Check also the &lt;a class="reference internal" href="#notebook"&gt;notebook&lt;/a&gt;
for an example of wavelet decomposition of the audio signal shown
above.&lt;/p&gt;
&lt;div class="figure align-center"&gt;
&lt;img alt="" src="images/Wavelets_-_Filter_Bank.png" /&gt;
&lt;p class="caption"&gt;&lt;em&gt;Filter bank used by a discrete wavelet transform with 3 levels of decomposition (image from the&lt;/em&gt; &lt;a class="reference external" href="http://en.wikipedia.org/wiki/File:Wavelets_-_Filter_Bank.png"&gt;WikiMedia Commons&lt;/a&gt; &lt;em&gt;)&lt;/em&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="section" id="sparse-coding-and-dictionary-based-methods"&gt;
&lt;h2&gt;Sparse coding and dictionary-based methods&lt;/h2&gt;
&lt;p&gt;Sparse signal approximations are the basis for a variety of signal
processing techniques. Such approximations are usually employed with
the objective of having a signal representation that is more
meaningful, malleable, and robust to noise than the ones obtained by
standard transform methods &lt;a class="citation-reference" href="#sturm" id="id5"&gt;[Sturm]&lt;/a&gt;. The so-called
dictionary-based methods (DBM) decompose a signal into a linear
combination of waveforms through an approximation technique such as
Matching Pursuit (MP) &lt;a class="citation-reference" href="#mallat1993" id="id6"&gt;[Mallat1993]&lt;/a&gt; or Orthogonal Matching Pursuit
(OMP) &lt;a class="citation-reference" href="#pati1993" id="id7"&gt;[Pati1993]&lt;/a&gt;. The collection of waveforms that can be
selected for the linear combination is called a dictionary. This
dictionary is usually overcomplete, either because it is formed by
merging complete dictionaries or because the functions are chosen
arbitrarily.&lt;/p&gt;
&lt;p&gt;I will talk more about sparse coding and dictionary-based methods
later, since sparse coding is one of the methods we'll see in the
course.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="ipython-notebook"&gt;
&lt;span id="notebook"&gt;&lt;/span&gt;&lt;h2&gt;IPython notebook&lt;/h2&gt;
&lt;p&gt;An IPython notebook with examples for all the representations
described here (except sparse coding) is available on my &lt;a class="reference external" href="https://github.com/jfsantos/ift6266h14"&gt;GitHub
repo&lt;/a&gt;. You will need to install the packages &lt;a class="reference external" href="http://www.pybytes.com/pywavelets/"&gt;PyWavelets&lt;/a&gt; and
&lt;a class="reference external" href="https://github.com/cournape/talkbox"&gt;scikits.talkbox&lt;/a&gt; (both are available at PyPI) to be able to run it. If you just want to take a look without interacting with the code, you can access it &lt;a class="reference external" href="http://nbviewer.ipython.org/github/jfsantos/ift6266h14/blob/master/notebooks/Speech%20representation%20examples.ipynb"&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="references"&gt;
&lt;h2&gt;References&lt;/h2&gt;
&lt;table class="docutils citation" frame="void" id="stevens2005" rules="none"&gt;
&lt;colgroup&gt;&lt;col class="label" /&gt;&lt;col /&gt;&lt;/colgroup&gt;
&lt;tbody valign="top"&gt;
&lt;tr&gt;&lt;td class="label"&gt;&lt;a class="fn-backref" href="#id1"&gt;[Stevens2005]&lt;/a&gt;&lt;/td&gt;&lt;td&gt;S. S. Stevens, J. Volkmann, and E. B. Newman, “A Scale for the Measurement of the Psychological Magnitude Pitch,” The Journal of the Acoustical Society of America, vol. 8, no. 3, pp. 185–190, Jun. 2005.&lt;/td&gt;&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;table class="docutils citation" frame="void" id="o1988linear" rules="none"&gt;
&lt;colgroup&gt;&lt;col class="label" /&gt;&lt;col /&gt;&lt;/colgroup&gt;
&lt;tbody valign="top"&gt;
&lt;tr&gt;&lt;td class="label"&gt;&lt;a class="fn-backref" href="#id2"&gt;[o1988linear]&lt;/a&gt;&lt;/td&gt;&lt;td&gt;D. O’Shaughnessy, “Linear predictive coding,” IEEE Potentials, vol. 7, no. 1, pp. 29–32, Feb. 1988.&lt;/td&gt;&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;table class="docutils citation" frame="void" id="valin2006speex" rules="none"&gt;
&lt;colgroup&gt;&lt;col class="label" /&gt;&lt;col /&gt;&lt;/colgroup&gt;
&lt;tbody valign="top"&gt;
&lt;tr&gt;&lt;td class="label"&gt;&lt;a class="fn-backref" href="#id3"&gt;[valin2006speex]&lt;/a&gt;&lt;/td&gt;&lt;td&gt;J.-M. Valin, “Speex: a free codec for free speech,” in Australian National Linux Conference, Dunedin, New Zealand, 2006.&lt;/td&gt;&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;table class="docutils citation" frame="void" id="mallat1989theory" rules="none"&gt;
&lt;colgroup&gt;&lt;col class="label" /&gt;&lt;col /&gt;&lt;/colgroup&gt;
&lt;tbody valign="top"&gt;
&lt;tr&gt;&lt;td class="label"&gt;&lt;a class="fn-backref" href="#id4"&gt;[mallat1989theory]&lt;/a&gt;&lt;/td&gt;&lt;td&gt;S. G. Mallat, “A theory for multiresolution signal decomposition: the wavelet representation,” Pattern Analysis and Machine Intelligence, IEEE Transactions on, vol. 11, no. 7, pp. 674–693, 1989.&lt;/td&gt;&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;table class="docutils citation" frame="void" id="sturm" rules="none"&gt;
&lt;colgroup&gt;&lt;col class="label" /&gt;&lt;col /&gt;&lt;/colgroup&gt;
&lt;tbody valign="top"&gt;
&lt;tr&gt;&lt;td class="label"&gt;&lt;a class="fn-backref" href="#id5"&gt;[Sturm]&lt;/a&gt;&lt;/td&gt;&lt;td&gt;B. L. Sturm, C. Roads, A. McLeran, and J. J. Shynk, “Analysis, Visualization, and Transformation of Audio Signals Using Dictionary-based Methods†,” Journal of New Music Research, vol. 38, no. 4, pp. 325–341, 2009.&lt;/td&gt;&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;table class="docutils citation" frame="void" id="mallat1993" rules="none"&gt;
&lt;colgroup&gt;&lt;col class="label" /&gt;&lt;col /&gt;&lt;/colgroup&gt;
&lt;tbody valign="top"&gt;
&lt;tr&gt;&lt;td class="label"&gt;&lt;a class="fn-backref" href="#id6"&gt;[Mallat1993]&lt;/a&gt;&lt;/td&gt;&lt;td&gt;S. G. Mallat and Z. Zhang, “Matching pursuits with time-frequency dictionaries,” IEEE Transactions on Signal Processing, vol. 41, no. 12, pp. 3397–3415, Dec. 1993.&lt;/td&gt;&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;table class="docutils citation" frame="void" id="pati1993" rules="none"&gt;
&lt;colgroup&gt;&lt;col class="label" /&gt;&lt;col /&gt;&lt;/colgroup&gt;
&lt;tbody valign="top"&gt;
&lt;tr&gt;&lt;td class="label"&gt;&lt;a class="fn-backref" href="#id7"&gt;[Pati1993]&lt;/a&gt;&lt;/td&gt;&lt;td&gt;Y. C. Pati, R. Rezaiifar, and P. S. Krishnaprasad, “Orthogonal matching pursuit: Recursive function approximation with applications to wavelet decomposition,” in Signals, Systems and Computers, 1993. 1993 Conference Record of The Twenty-Seventh Asilomar Conference on, 1993, pp. 40–44.&lt;/td&gt;&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
</summary><category term="ift6266"></category></entry><entry><title>Personal research journal: deep learning for speech synthesis</title><link href="/intro_ift6266.html" rel="alternate"></link><updated>2014-01-25T15:00:00-05:00</updated><author><name>jfsantos</name></author><id>tag:,2014-01-25:intro_ift6266.html</id><summary type="html">&lt;p&gt;This is the introduction to a series of reports on my experiments on
deep learning methods for speech synthesis. These experiments are part
of my coursework for Dr. Yoshua Bengio's &lt;a class="reference external" href="http://ift6266h14.wordpress.com"&gt;Representation Learning&lt;/a&gt;
course at Université de Montréal. All the related code is going to be
posted at a &lt;a class="reference external" href="https://github.com/jfsantos/ift6266h14"&gt;GitHub repository&lt;/a&gt; as well.&lt;/p&gt;
&lt;p&gt;Please visit the &lt;a class="reference external" href="/tag/ift6266.html"&gt;ift6266 tag page&lt;/a&gt; for a list of all the posts
related to this project.&lt;/p&gt;
</summary><category term="ift6266"></category></entry></feed>